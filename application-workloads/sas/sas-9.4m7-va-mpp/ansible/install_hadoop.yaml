#
# Copyright (c) 2020, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
#
---
- name: Distribute ssh key to Hadoop nodes for Primary User
  hosts: [va_controllers,va_workers,midtier_servers]
  become_user: AzureUser
  become: yes
  tasks:
  - name: Copy ssh key from jumpvm to Hadoop nodes for Primary User
    copy:
      src: ~/.ssh
      dest: $HOME
      mode: preserve

- name: Distribute ssh key to Hadoop nodes for sasinst
  hosts: [va_controllers,va_workers,midtier_servers]
  become_user: sasinst
  become: yes
  tasks:
  - name: Copy ssh key from jumpvm to Hadoop nodes for sasinst
    copy:
      src: ~/.ssh
      dest: $HOME
      mode: preserve

- name: Distribute ssh key to Hadoop nodes for sasinst
  hosts: [va_controllers,va_workers,midtier_servers]
  become_user: sassrv
  become: yes
  tasks:
  - name: Copy ssh key from jumpvm to Hadoop nodes for sasinst
    copy:
      src: ~/.ssh
      dest: $HOME
      mode: preserve

- name: Installing all updates with zypper
  hosts: [va_controllers,va_workers]
  become_user: root
  become: yes
  gather_facts: false
  tasks:

  - name: Install required packages
    package:
      name: "{{ item }}"
      state: latest
    loop:
      - java-1_8_0-openjdk

- name: Download Hadoop on nameNode and dataNodes
  hosts: [va_controllers,va_workers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Make hadoop home directory
    become: yes
    file:
      path: "{{ HADOOP_HOME }}"
      state: directory
      mode: '0755'

  - name: Downloading Hadoop from Apache
    become: yes
    unarchive:
      src: "/sasshare/hadoop-{{ HADOOP_VERSION }}.tar.gz"
      dest: "{{ HADOOP_HOME }}"
      extra_opts: [--strip-components=1]
    register: task_result
    until: task_result is success
    retries: 10
    delay: 5

- name: Setup environment on nameNode (master)
  hosts: [va_controllers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Insert Hadoop environment into bashrc
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      dest: "$HOME/.bashrc"
      backup: yes
      block: |
        export HADOOP_HOME={{ HADOOP_HOME }}
        export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
        export HADOOP_MAPRED_HOME=$HADOOP_HOME
        export HADOOP_COMMON_HOME=$HADOOP_HOME
        export HADOOP_HDFS_HOME=$HADOOP_HOME
        export YARN_HOME=$HADOOP_HOME
        export HDFS_NAMENODE_USER="sasinst"
        export HDFS_DATANODE_USER="sasinst"
        export HDFS_SECONDARYNAMENODE_USER="sasinst"
        export YARN_RESOURCEMANAGER_USER="sasinst"
        export YARN_NODEMANAGER_USER="sasinst"
        export JAVA_HOME=/usr/lib64/jvm/jre-1.8.0-openjdk
        export PATH=$JAVA_HOME:$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
      insertafter: EOF
      create: yes

  - name: Edit hadoop-env.sh configuration
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/hadoop-env.sh"
      insertafter: "# export JAVA_HOME="
      block: |
        export JAVA_HOME=/usr/lib64/jvm/jre-1.8.0-openjdk

  - name: Edit core-site.xml configuration
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/core-site.xml"
      insertafter: "<configuration>"
      block: |
        <property>
          <name>fs.default.name</name>
          <value>hdfs://{{ inventory_hostname }}:9000</value>
        </property>

  - name: Edit hdfs-site.xml configuration
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/hdfs-site.xml"
      insertafter: "<configuration>"
      block: |
        <property>
          <name>dfs.namenode.name.dir</name>
          <value>{{ sasFolder }}/hadoop_data/nameNode</value>
        </property>
        <property>
          <name>dfs.datanode.data.dir</name>
          <value>{{ sasFolder }}/hadoop_data/dataNode</value>
        </property>
        <property>
          <name>dfs.replication</name>
          <value>2</value>
        </property>

  - name: Edit mapred-site.xml configuration
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/mapred-site.xml"
      insertafter: "<configuration>"
      block: |
        <property>
          <name>mapreduce.framework.name</name>
          <value>yarn</value>
        </property>
        <property>
          <name>yarn.app.mapreduce.am.env</name>
          <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
          <name>mapreduce.map.env</name>
          <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
          <name>mapreduce.reduce.env</name>
          <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
          <name>yarn.app.mapreduce.am.resource.mb</name>
          <value>512</value>
        </property>
        <property>
          <name>mapreduce.map.memory.mb</name>
          <value>256</value>
        </property>
        <property>
          <name>mapreduce.reduce.memory.mb</name>
          <value>256</value>
        </property>

  - name: Edit yarn-site.xml configuration
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/yarn-site.xml"
      insertafter: "<configuration>"
      block: |
        <property>
          <name>yarn.acl.enable</name>
          <value>0</value>
        </property>
        <property>
          <name>yarn.resourcemanager.hostname</name>
          <value>{{ ansible_eth0.ipv4.address }}</value>
        </property>
        <property>
          <name>yarn.nodemanager.aux-services</name>
          <value>mapreduce_shuffle</value>
        </property>
        <property>
          <name>yarn.nodemanager.resource.memory-mb</name>
          <value>2048</value>
        </property>
        <property>
          <name>yarn.scheduler.maximum-allocation-mb</name>
          <value>2048</value>
        </property>
        <property>
          <name>yarn.scheduler.minimum-allocation-mb</name>
          <value>1024</value>
        </property>
        <property>
          <name>yarn.nodemanager.vmem-check-enabled</name>
          <value>false</value>
        </property>
        # The last property disables virtual-memory checking which can prevent containers from being allocated properly with openjdk if enabled.
        # Note: Memory allocation can be tricky on low RAM nodes because default values are not suitable for nodes with less than 8GB of RAM.
        #       We have manually set memory allocation for MapReduce jobs, and provide a sample configuration for 4GB RAM nodes.

  - name: Make sure localhost is not in the workers file
    lineinfile:
      path: "{{ HADOOP_HOME }}/etc/hadoop/workers"
      state: absent
      regexp: '^localhost'

  - name: Add worker nodes to workers file
    become: yes
    lineinfile:
      path: "{{ HADOOP_HOME }}/etc/hadoop/workers"
      line: "{{ hostvars[item]['inventory_hostname']}}"
      state: present
      backup: yes
    with_items: "{{ groups['va_workers'] }}"

- name: Distribute Hadoop configuration to work nodes
  hosts: [va_workers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Copy Hadoop configuration files to worker nodes
    synchronize: 
      src: "{{ sasFolder }}/hadoop/etc/hadoop/"
      dest: "{{ sasFolder }}/hadoop/etc/hadoop"
    delegate_to: vacontroller

#- name: Start Hadoop
#  hosts: [va_controllers]
#  become_user: sasinst
#  become: yes
#  vars_files:
#    - /tmp/ansible_vars.yaml
#  tasks:
#  - name: Format hdfs on namenode
#    become: yes
#    shell: |-
#      {{ HADOOP_HOME }}/bin/hdfs namenode -format
#
#  - name: Start dfs
#    become: yes
#    shell: |
#      {{ HADOOP_HOME }}/bin/hdfs --daemon start namenode
#      {{ HADOOP_HOME }}/bin/hdfs --daemon start datanode
#
#  - name: Start yarn
#    become: yes
#    shell: |
#      {{ HADOOP_HOME }}/bin/yarn --daemon start resourcemanager
#      {{ HADOOP_HOME }}/bin/yarn --daemon start nodemanager
#      {{ HADOOP_HOME }}/bin/yarn --daemon start proxyserver
