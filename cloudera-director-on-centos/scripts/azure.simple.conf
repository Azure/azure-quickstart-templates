#
# Copyright (c) 2016 Cloudera, Inc. All rights reserved.
#

#
# Sample Cloudera Director configuration file based on the Cloudera Azure reference architecture:
# http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# Simple non-HA cluster with single master node, 3 worker nodes
#

#
# Cluster name
#

name: C5-Simple-Azure

#
# Cloud provider configuration (credentials, region, and
# management/authentication endpoints)
#

provider {
    type: azure

    #
    # ID of Azure region to use. NOTE: region must support Premium Storage
    # See: https://azure.microsoft.com/en-us/regions/#services
    #

    region: eastus

    #
    # Azure Resource Management URL.
    #

    mgmtUrl: "https://management.core.windows.net/"

    #
    # Azure Active Directory Subscription ID.
    #

    subscriptionId: REPLACE-ME

    #
    # Azure Active Directory URL.
    #

    aadUrl: "https://login.windows.net/"

    #
    # Tenant ID (from AAD)
    #

    tenantId: REPLACE-ME

    #
    # Azure Active Directory Application Client ID.
    #

    clientId: REPLACE-ME

    #
    # Client Secret
    #

    clientSecret: "REPLACE-ME"

}

#
# SSH credentials to use to connect to the machines
#

ssh {
    username: REPLACE-ME
    privateKey: """-----BEGIN RSA PRIVATE KEY-----
REPLACE-ME
-----END RSA PRIVATE KEY-----"""
}

# Instant Templates

instances {

    master {

        #
        # The image ID used for instances is an alias defined in the plugin configuration file
        #

        image: cloudera-centos-6-latest

        #
        # VM type.
        # NOTE: Per Azure RA, only DS13 & DS14 are supported.
        #

        type: STANDARD_DS14

        #
        # The Resource Group for the Network Security Group. The Resource Group you specify must
        # exist within the region you selected and should be the same for all instances
        # that will be used in the same cluster.
        # See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
        #

        networkSecurityGroupResourceGroup: REPLACE-ME

        #
        # The Network Security Group for this instance type, this has to be within the
        # networkSecurityGroupResourceGroup. NSG configuration allows you to limit access
        # to the VM with firewall-like rules.
        # See: https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/
        #

        networkSecurityGroup: REPLACE-ME

        #
        # The Resource Group for the Virtual Network. The Resource Group you specify must
        # exist within the region you selected and should be the same for all instances
        # that will be used in the same cluster.
        # See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
        #

        virtualNetworkResourceGroup: REPLACE-ME

        #
        # The Azure Virtual Network that will be used, this has to be within the
        # virtualNetworkResourceGroup and should be the same for all instances
        # that will be used in the same cluster.
        # See: https://azure.microsoft.com/en-us/documentation/services/virtual-network/
        #

        virtualNetwork: REPLACE-ME

        #
        # The name of the Subnet that will be used, this has to be within the virtualNetwork.
        #

        subnetName: REPLACE-ME

        #
        # Resource Group for the deployment.  The Resource Group you specify must
        # exist within the region you selected and should be the same for all instances
        # that will be used in the same cluster.
        # See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
        #

        computeResourceGroup: REPLACE-ME

        #
        # hostname Fqdn Suffix.  VNET must be configured to allow DNS updates for this domain.
        #
        hostFqdnSuffix: "REPLACE-ME"

        #
        # Availability Set for this instance type.  Machines within the same availability set will have staggared
        # maintanance times.  With a default availability set configuration no more than 1/5 machines will be offline
        # at a time (rounded up).
        # See: https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-manage-availability/
        #

        availabilitySet: asmaster

        #
        # Should this instance type have Azure Public IP Address and DNS Label?  If Yes, the
        # machine will have a publically resolvable hostname  like:
        #       {instanceNamePrefix}{UUID}.{region}.coudapp.azure.com
        #

        publicIP: No

        #
        # prefix for instance name and hostname for this intance type
        #

        instanceNamePrefix: cdmstr

        #
        # Data drives. All drives will be 1 TB
        # Data drives are mounted on /data0 .. /data[n]
        # /data0 - Dedicated Log Device
        #
        # For Masters:
        # /data1 - NameNode Data
        # /data2 - Zookeeper Data / DataLog
        # /data3 - HDFS Secondary NameNode Data
        #
        # For Workers /data1 .. /data[n] will be used for HDFS data
        #

        dataDiskCount: 4

        tags {
            owner: ${?USER}
        }

        #
        # Bootstrap script will be run after the VM is initialized.  This should
        # be used to set up preconditions for successful installation of CDH
        # components.
        # The example below configures a dhclient hook to register the A record
        # and PTR record with the DNS server configured for the VNET to satisfy
        # proper forward and reverse DNS resolution.
        # Azure's default DNS currently does not support Reverse Lookup on private
        # IP Addresses, which is a requirement for CDH. See the following link for
        # an example BIND setup to satisfy this requirement:
        # http://www.cloudera.com/documentation/director/latest/topics/director_get_started_azure_ddns.html
        #

        bootstrapScript: """#!/bin/sh
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
   [ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # REPLACE-ME If no hostname is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
"""

    }

    worker {
        image: cloudera-centos-6-latest
        type: STANDARD_DS14
        networkSecurityGroupResourceGroup: REPLACE-ME
        networkSecurityGroup: REPLACE-ME
        virtualNetworkResourceGroup: REPLACE-ME
        virtualNetwork: REPLACE-ME
        subnetName: REPLACE-ME
        computeResourceGroup: REPLACE-ME
        hostFqdnSuffix: "REPLACE-ME"
        availabilitySet: asworker
        publicIP: No
        instanceNamePrefix: cdwork
        dataDiskCount: 11
        tags {
            owner: ${?USER}
        }
        bootstrapScript: """#!/bin/sh
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
   [ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # REPLACE-ME If no hostname is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
"""
    }

    edge {
        image: cloudera-centos-6-latest
        type: STANDARD_DS14
        networkSecurityGroupResourceGroup: REPLACE-ME
        networkSecurityGroup: REPLACE-ME
        virtualNetworkResourceGroup: REPLACE-ME
        virtualNetwork: REPLACE-ME
        subnetName: REPLACE-ME
        computeResourceGroup: REPLACE-ME
        hostFqdnSuffix: "REPLACE-ME"
        availabilitySet: asedge
        publicIP: No
        instanceNamePrefix: cdedge
        dataDiskCount: 1
        tags {
            owner: ${?USER}
        }
        bootstrapScript: """#!/bin/sh
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
   [ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # REPLACE-ME If no hostname is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
"""
    }

} # End instance templates

#
# Required external database server configuration.
#
# Cloudera Director can create databases on existing database servers.
# NOTE: Cloudera does not support Azure SQL DB service.
#

databaseServers {

    mysqlprod1 {
        type: mysql
        host: REPLACE-ME # Recommend static IP address of database server
        port: 3306
        user: REPLACE-ME
        password: REPLACE-ME
    }

#    existingpostgres1 {
#        type: postgresql
#        host: REPLACE-ME # Recommend static IP address of database server
#        port: 5432
#        user: REPLACE-ME
#        password: REPLACE-ME
#    }

} # End external database configs

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.edge} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    #
    # Licensing configuration
    #
    # There are three mutually exclusive options for setting up Cloudera Manager's license.
    # 1. License text may be embedded in this file using the "license" field. Triple quotes (""")
    #    are recommended for including multi-line text strings.
    # 2. The "licensePath" can be used to specify the path to a file containing the license.
    # 3. The "enableEnterpriseTrial" flag indicates whether the 60-Day Cloudera Enterprise Trial
    #    should be activated when no license is present. This must not be set to true if a
    #    license is included using either "license" or "licensePath".

    #
    # Embed a license for Cloudera Manager
    #

    # license: """
    #   -----BEGIN PGP SIGNED MESSAGE-----
    #   Hash: SHA1
    #
    # {
    #   "version"        : 1,
    #   "name"           : "License Owner",
    #   "uuid"           : "license id",
    #   "expirationDate" : 0,
    #   "features"       : [ "FEATURE1", "FEATURE2" ]
    # }
    # -----BEGIN PGP SIGNATURE-----
    # Version: GnuPG v1.4.11 (GNU/Linux)
    #
    # PGP SIGNATURE
    # -----END PGP SIGNATURE-----
    # """

    #
    # Include a license for Cloudera Manager from an external file
    #

    # licensePath: "/path/to/license.txt.asc"

    #
    # Specify the billingId.
    #
    # Cloudera Director will use the billing ID to report usage information to a metering service
    # for usage based billing.
    #
    # Usage reporting starts as soon as you assign a billing ID and a license to a Cloudera Manager.
    # If you remove a billing ID, Director will stop reporting to the metering service.
    #
    # When usage reporting stops, you will not have access to Cloudera Support with this deployment.
    # If you want a billing ID, please contact Cloudera.
    #

    # billingId: REPLACE-ME

    #
    # Activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

    #
    # Install unlimited strength JCE policy files along with Cloudera Manager
    #

    # unlimitedJce: true

    #
    # Optional database configuration
    #
    # There are three mutually exclusive options for database usage in Cloudera Director.
    # 1. This option is NOT supported for production use.
    #    With no configuration, an embedded PostgreSQL database will be used.
    # 2. Alternatively, existing external databases can be used.
    # 3. Finally, databases can be created on the fly on existing external database servers.

    #
    # Optional configuration for existing external databases
    #
    # databases {
    #     CLOUDERA_MANAGER {
    #         type: postgresql
    #
    #         host: db.example.com
    #         port: 123
    #
    #         user: admin
    #         password: 1231ed
    #
    #         name: scm
    #     }
    #
    #     ACTIVITYMONITOR { ... }
    #
    #     REPORTSMANAGER { ... }
    #
    #     NAVIGATOR { ... }
    #
    #     # Added in Cloudera Manager 5.2+
    #     NAVIGATORMETASERVER { ... }
    # }

    #
    # Optional configuration for creating external databases on the fly
    #
    # When a database is created on the fly, Director generates a random database name using the specified database
    # name prefix, a random username based on the specified username prefix, and a random password. The password is
    # stored by Director and made available to the service that uses the database. If multiple services reference the
    # same external database server, Director will create a database for each.
    #
    # MySQL limits usernames to sixteen characters. Therefore, limit usernamePrefix values for databases on MySQL to
    # seven characters; the remaining nine characters are used by the randomized suffix generated by Director.
    #

    databaseTemplates {
        CLOUDERA_MANAGER {
            name: cmtemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: scm
            usernamePrefix: cmadmin
        }

        ACTIVITYMONITOR {
            name: amontemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: amon
            usernamePrefix: amadmin
        }

        REPORTSMANAGER {
            name: rmantemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: rman
            usernamePrefix: rmadmin
        }

        NAVIGATOR {
            name: navtemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: nav
            usernamePrefix: nadmin
        }

        # Added in Cloudera Manager 5.2+
        NAVIGATORMETASERVER {
            name: navmetatemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: navmeta
            usernamePrefix: nmadmin
        }
    }

    #
    # Configuration to override Cloudera Manager package repositories
    #

    # repository: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5.5/"
    # repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/RPM-GPG-KEY-cloudera"

    # OR use an existing Cloudera Manager installation

    # hostname: "192.168.33.10"
    # username: <if not default 'admin'>
    # password: <if not default 'admin'>

    #
    # Optional configuration for Cloudera Manager and its management services
    #
    # Configuration properties for CLOUDERA_MANAGER are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
    #
    # Configuration properties for the Cloudera Management services are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_mgmtservice.html
    #
    # Configuration properties for Hosts are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_host.html
    #

    configs {
            # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options
            CLOUDERA_MANAGER {
                # enable_api_debug: false
                custom_banner_html: "Managed by Cloudera Director"
            }

            # CLOUDERA_MANAGEMENT_SERVICE corresponds to the Service-Wide configuration options
            CLOUDERA_MANAGEMENT_SERVICE {
                # enable_alerts : false
                # enable_config_alerts : false
            }

             SERVICEMONITOR {
                 mgmt_log_dir:/data0/log/cloudera-scm-firehose
                 firehose_storage_dir:/data0/lib/cloudera-service-monitor
             }

             ACTIVITYMONITOR {
                 mgmt_log_dir:/data0/log/cloudera-scm-firehose
             }

             HOSTMONITOR {
                 mgmt_log_dir: /data0/log/cloudera-scm-firehose
                 firehose_storage_dir: /data0/lib/cloudera-host-monitor
             }

             REPORTSMANAGER {
                 headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
                 mgmt_log_dir: /data0/log/cloudera-scm-headlamp
             }

             EVENTSERVER {
                 mgmt_log_dir:/data0/log/cloudera-scm-eventserver
                 eventserver_index_dir:/data0/lib/cloudera-scm-eventserver
             }

             ALERTPUBLISHER {
                 mgmt_log_dir:/data0/log/cloudera-scm-alertpublisher
             }

             NAVIGATOR {
                 mgmt_log_dir:/data0/log/cloudera-scm-navigator
             }

             NAVIGATORMETASERVER {
                 audit_event_log_dir:/data0/log/cloudera-scm-navigator/audit
                 data_dir:/data0/lib/cloudera-scm-navigator
                 mgmt_log_dir:/data0/log/cloudera-scm-navigator
             }

             # Configuration properties for all hosts
             HOSTS { }
         }
    } # End CM configuration

#
# Simple single-master Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
        CDH: 5
    }

    #
    # Optional override of CDH parcel repositories
    #

    # parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.5/"]

    services: [HDFS, YARN, ZOOKEEPER, HBASE, HIVE, HUE, IMPALA, OOZIE, SPARK_ON_YARN]

    #
    # Optional custom service configurations
    # Configuration keys containing periods must be enclosed in double quotes.
    #

    configs {
        # HDFS fencing should be set to true for HA configurations
        HDFS {
            dfs_replication: "3"
            dfs_block_local_path_access_user: "impala,hbase,mapred,spark"
        }

        HIVE {
            audit_event_log_dir: /data0/log/hive/audit
            lineage_event_log_dir: /data0/log/hive/lineage
        }

        HBASE {
            audit_event_log_dir: /data0/log/hbase/audit
        }
    }

    #
    # Optional configuration for existing external database for Hive Metastore, Hue,
    # and Oozie databases
    #

    # databases {
    #     HIVE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hive
    #         password: pass
    #         name: hive_db
    #     }
    #     HUE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hue
    #         password: pass
    #         name: hue_db
    #     }
    #     OOZIE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: oozie
    #         password: pass
    #         name: oozie_db
    #     }
    # }

    #
    # Optional configuration for creating external database on the fly for Hive Metastore
    # Hue, and Oozie databases
    #

    databaseTemplates: {
        HIVE {
            name: hivetemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: hivemetastore
            usernamePrefix: hive
        }

        HUE {
            name: huetemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: huedb
            usernamePrefix: hue
        }

        OOZIE {
            name: oozietemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: ooziedb
            usernamePrefix: oozie
        }
    }

    #
    # This reference configuration follows the Cloudera Azure Reference Architecture, modified
    # for a simple single-master cluster.
    #

    masters {
        count: 1

        instance: ${instances.master} {
            tags {
                group: masters
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            HBASE: [MASTER, HBASETHRIFTSERVER] # Alternately [HBASERESTSERVER], for HUE Integration
            HUE: [HUE_SERVER]
            OOZIE: [OOZIE_SERVER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
        }

        configs {
            HDFS {
                NAMENODE {
                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data1/dfs/nn
                }
                SECONDARYNAMENODE {
                    secondarynamenode_log_dir: /data0/log/hadoop-hdfs
                    fs_checkpoint_dir_list: /data3/dfs/snn
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
              JOBHISTORY {
                  mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
              }
            }
            HBASE {
              MASTER {
                  hbase_master_log_dir: /data0/log/hbase
              }
              HBASETHRIFTSERVER {
                  hbase_thriftserver_log_dir: /data0/log/hbase
              }
              #HBASERESTSERVER {
              #    hbase_restserver_log_dir: /data0/log/hbase
              #}
            }
            HIVE {
                HIVEMETASTORE {
                    hive_log_dir: /data0/log/hive
                }
                HIVESERVER2 {
                    hive_log_dir: /data0/log/hive
                }
                WEBHCAT {
                    hcatalog_log_dir: /data0/log/hcatalog
                }
            }
            OOZIE {
                OOZIE_SERVER {
                    oozie_plugins_list: "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService"
                    oozie_log_dir: /data0/log/oozie
                }
            }
            HUE {
                HUE_SERVER {
                    hue_server_log_dir: /data0/log/hue
                }
            }
            IMPALA {
                CATALOGSERVER {
                    log_dir: /data0/log/catalogd
                }
                STATESTORE {
                    log_dir: /data0/log/statestore
                }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            #HBASE {
            #}
        }
    }

    workers {
        count: 3
        #
        # Minimum number of instances required to set up the cluster.
        # Fail and quit if minCount number of instances is not available in this cloud
        # environment. Else, continue setting up the cluster.
        #
        minCount: 3

        instance: ${instances.worker} {
            tags {
                group: worker
            }
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            HBASE: [REGIONSERVER]
            IMPALA: [IMPALAD]
        }

        # Optional custom role configurations
        # Configuration keys containing periods must be enclosed in double quotes.
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn,/data4/dfs/dn,/data5/dfs/dn,/data6/dfs/dn,/data7/dfs/dn,/data8/dfs/dn,/data9/dfs/dn,/data10/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: /data0/log/hadoop-yarn/container
                    yarn_nodemanager_local_dirs: "/data1/yarn,/data2/yarn,/data3/yarn,/data4/yarn,/data5/yarn,/data6/yarn,/data7/yarn,/data8/yarn,/data9/yarn,/data10/yarn"
                }
            }
            HBASE {
                REGIONSERVER {
                    hbase_regionserver_log_dir: /data0/log/hbase
                }
            }
            IMPALA {
                IMPALAD {
                    log_dir: /data0/log/impalad
                    lineage_event_log_dir: /data0/log/impalad/lineage
                    audit_event_log_dir: /data0/log/impalad/audit
                    scratch_dirs: "/data1/impala/impalad,/data2/impala/impalad,/data3/impala/impalad,/data4/impala/impalad,/data5/impala/impalad,/data6/impala/impalad,/data7/impala/impalad,/data8/impala/impalad,/data9/impala/impalad,/data10/impala/impalad"
                }
            }
        }
    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Hello World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple post-creation scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Hello again!'
    """]

    # For more complex scripts, post creation scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous postCreateScripts section.
    # postCreateScriptsPaths: ["/tmp/test-script.sh",
    #                         "/tmp/test-script.py"]

    preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

    # For more complex scripts, pre terminate scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous preTerminateScripts section.
    # preTerminateScriptsPaths: ["/tmp/test-script.sh",
    #                            "/tmp/test-script.py"]
}
