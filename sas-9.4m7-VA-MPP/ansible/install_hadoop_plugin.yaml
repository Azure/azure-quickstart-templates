#
# Copyright (c) 2020, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
#
---
- name: Prepare SAS Plug-in for Hadoop install
  hosts: [va_controllers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Find version of plugin
    find:
      paths: /sasshare/depot/standalone_installs/SAS_Plug-ins_for_Hadoop
      file_type: directory
    register: version

  - name: Extracting SAS Plug-in for Hadoop from external file
    become: yes
    unarchive:
      remote_src: yes
      src: "{{ item.path }}/Linux_for_x64/hdatplugins.tar.gz"
      dest: /tmp
    with_items: "{{ version.files }}"
    register: task_result
    until: task_result is success
    retries: 10
    delay: 5

  - name: Make SAS Plug-in home dirctory
    become: yes
    file:
      path: "{{ HDAT_HOME }}"
      state: directory
      mode: '0755'

  - name: Run SAS Plug-in install
    become: yes
    environment:
      HADOOP_HOME: "{{ HADOOP_HOME }}"
      PATH: "{{ HADOOP_HOME }}/bin:{{ ansible_env.PATH }}"
    expect:
      echo: yes
      timeout: 180
      command: "/tmp/hdatplugins/sashdat-install.sh -add -hostfile /etc/grid.hosts -hdathome {{ HDAT_HOME }}"
      responses:
        'Enter Yes\(y\) to continue': 'y'

  - name: Edit hdfs-site.xml configuration
    blockinfile:
      backup: yes
      marker: "## {mark} added by ansible (install_hadoop_plugin)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/hdfs-site.xml"
      insertafter: "<configuration>"
      block: |
        <property>
          <name>com.sas.cas.service.allow.put</name>
          <value>true</value>
        </property>

  - name: Modify hdfs-site.xml after plug-in install
    replace:
      path: "{{ HADOOP_HOME }}/etc/hadoop/hdfs-site.xml"
      replace: "<value>{{ HDAT_HOME }}/HDATHome/bin/sascasfd</value>"
      regexp: "<value>{{ sasFolder }}/HDATHome/bin/sascasfd</value>"

- name: Update the hadoop classpath
  hosts: [va_controllers,va_workers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Edit hadoop-env.sh classpath
    blockinfile:
      marker: "## {mark} added by ansible (install_hadoop_plugin)"
      path: "{{ HADOOP_HOME }}/etc/hadoop/hadoop-env.sh"
      insertafter: "# export HADOOP_CLASSPATH="
      block: |
        export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:{{ HDAT_HOME }}/HDATHome/lib/*

- name: Distribute Hadoop configuration to work nodes
  hosts: [va_workers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Copy hdfs-site.xml file to worker nodes
    synchronize:
      src: "{{ HADOOP_HOME }}/etc/hadoop/hdfs-site.xml"
      dest: "{{ HADOOP_HOME }}/etc/hadoop/hdfs-site.xml"
    delegate_to: vacontroller

- name: Restart HDFS service and complete Hadoop configuration for SAS VA
  hosts: [va_controllers]
  become_user: sasinst
  become: yes
  vars_files:
    - /tmp/ansible_vars.yaml
  tasks:
  - name: Stop dfs
    become: yes
    shell: |-
      {{ HADOOP_HOME }}/sbin/stop-dfs.sh
      
  - name: Start dfs
    become: yes
    shell: |-
      {{ HADOOP_HOME }}/sbin/start-dfs.sh

  - name: Hadoop Configuration Step for SAS Visual Analytics
    become: yes
    command: "{{ item }}"
    with_items:
      - "{{ HADOOP_HOME }}/bin/hadoop fs -mkdir /hps"
      - "{{ HADOOP_HOME }}/bin/hadoop fs -mkdir /vapublic"
      - "{{ HADOOP_HOME }}/bin/hadoop fs -chmod 777 /hps"
      - "{{ HADOOP_HOME }}/bin/hadoop fs -chmod 1777 /vapublic"

## Maybe install Amabri (https://ambari.apache.org/), silent option
## amabri-server setup -s
